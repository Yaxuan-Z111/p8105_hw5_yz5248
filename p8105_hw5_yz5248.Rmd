---
title: "p8105_hw5_yz5248"
author: "yz5248"
date: "2025-11-02"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(broom)
library(readr)
```

# Problem 1
```{r}
birthdays = sample(1:365, 5, replace = TRUE)

repeated_bday = length(unique(birthdays)) < 5
repeated_bday
```

```{r}
bday_sim = function(n_room){
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n_room
  repeated_bday
}
```

```{r}
bday_sim_results =
  expand_grid(
    bdays = 2:50,
    iter = 1:10000
  ) |>
  mutate(
    result = map_lgl(bdays, bday_sim)
  ) |>
  group_by(
    bdays
  ) |>
  summarise(
    prob_repeat = mean(result)
  )
```

```{r}
bday_sim_results |>
  ggplot(aes(x = bdays, y = prob_repeat)) + 
  geom_point() +
  geom_line()
bday_sim_results
```
The simulation plot demonstrates that as group size increases, the probability of shared birthdays grows rapidly, reaching about 50% around 23 people and approaching 100% by 50 people — a clear empirical verification of the birthday paradox.

# Problem 2
```{r}
one_sample_sim = function(mu, sigma = 5, n_subj = 30, n_sim = 5000, conf_level = 0.95) {
  sim_df = replicate(n_sim, {
    x = rnorm(n_subj, mean = mu, sd = sigma)
    test_result = t.test(x, mu = 0, conf.level = conf_level)
    tidy_res = broom::tidy(test_result)
    tibble(
      mu_hat = mean(x),
      p_value = tidy_res$p.value
    )
  }, 
  simplify = FALSE)
  
  bind_rows(sim_df)
}
```

```{r}
sim_results_df =
  expand_grid(
    mu = 1:6,
    iter = 1:6
  ) |> 
  mutate(
    results = map(mu, one_sample_sim)
  ) |> 
  unnest(results) |> 
  group_by(mu) |> 
  summarise(
    power = mean(p_value < 0.05),
    mean_mu_hat = mean(mu_hat),
    mean_mu_hat_reject = mean(mu_hat[p_value < 0.05])
  )
sim_results_df
```

## plot 1
```{r}
p_power = sim_results_df |>
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  geom_point()
p_power
```
The plot shows a positive association between effect size and power:    
As effect size increases, the power of the test also increases.  
When effect size equals to 0 the power is very low, meaning the test rarely rejects the null hypothesis when it is true.    
As effect size increases to about 2, the power rises sharply (around 0.5), indicating the test becomes more capable of detecting the effect.  
## plot 2
```{r}
estimate_mu = sim_results_df |>
  ggplot(aes(x = mu)) + 
  geom_line(aes(y = mean_mu_hat, color = "All samples")) +
  geom_point(aes(y = mean_mu_hat, color = "All samples")) +
  geom_line(aes(y = mean_mu_hat_reject, color = "Rejected samples")) +
  geom_point(aes(y = mean_mu_hat_reject, color = "Rejected samples")) 

estimate_mu
```
From the plot, we could find that the blue line is systematically higher than the red line for small mu values. The average mu across all samples is approximately unbiased for the true mu, but the average mu among samples where the null is rejected is biased upward, especially for small mu. This happens because we condition on a rare event (rejection), which tends to occur when the observed effect size is large — demonstrating selection bias in inference after testing.

# Problem 3
```{r}
homicides_df = read_csv("homicide-data.csv")
```
The dataset compiled by The Washington Post contains detailed information on homicides that occurred in 50 of the largest U.S. cities between January 1, 2007, and December 31, 2017. Each row in the dataset represents a single homicide case. The data were collected from local police departments and publicly available police records, and later curated by The Washington Post for public use.  
The dataset contains variables: "uid", "reported_date", "victim_last", "victim_first", "victim_race", "victim_age", "victim_sex", "city", "state", "lat", "lon", "disposition"  

```{r}

city_state = homicides_df |>
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = if_else(
      disposition %in% c("Closed without arrest", "Open/No arrest"),
      1, 0
    )
  ) |>
  group_by(city_state) |>
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(unsolved),
    .groups = "drop"
  ) |>
  view()
```

```{r}
baltimore_df = city_state |>
  filter(city_state == "Baltimore, MD")

baltimore_test = prop.test(
  x = baltimore_df$unsolved_homicides,
  n = baltimore_df$total_homicides
) 
baltimore_test

baltimore_table = baltimore_test |>
  broom::tidy()
baltimore_table

```

```{r}
city_results = 
  city_state |>
  mutate(
    test_result = map2(unsolved_homicides, total_homicides, ~prop.test(.x, .y))
  )

city_results_tidy = 
  city_results |>
  mutate(
    tidy_res = map(test_result, broom::tidy)
  ) |>
  unnest(tidy_res) |> 
  select(city_state, total_homicides, unsolved_homicides, estimate, conf.low, conf.high)
city_results_tidy
```

```{r}
city_results_tidy |>
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) |>
  ggplot(aes(x = estimate, y = city_state)) +
  geom_point() +
  geom_errorbar(
    aes(xmin = conf.low, xmax = conf.high),
    width = 0.2, color = "gray40"
  ) +
  theme_minimal(base_size = 13)

```

